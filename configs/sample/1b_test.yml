model: meta-llama/Llama-3.2-1B-Instruct

output: outputs/1b_test.jsonl
split: test
B: 16

M: 1
temperature: 0.7
max_new_tokens: 512
