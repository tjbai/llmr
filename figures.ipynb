{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disclosure: much of this is generated by claude for the sake of pretty figures and quick analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# examine outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "    \n",
    "def evaluate_routing(small_file, large_file, small_proportion, n_trials= 10, seed=42):\n",
    "    with open(small_file) as f: small_outputs = [json.loads(line) for line in f]\n",
    "    with open(large_file) as f: large_outputs = [json.loads(line) for line in f]\n",
    "        \n",
    "    n_examples = len(small_outputs)\n",
    "    n_to_small = int(small_proportion * n_examples)\n",
    "    \n",
    "    pass_rates = []\n",
    "    perfect_rates = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        small_set = set(rng.choice(n_examples, n_to_small, replace=False))\n",
    "        \n",
    "        total_test_cases = 0\n",
    "        passed_test_cases = 0\n",
    "        perfect_solutions = 0\n",
    "        \n",
    "        for i in range(n_examples):\n",
    "            results = small_outputs[i]['results'] if i in small_set else large_outputs[i]['results']\n",
    "            \n",
    "            for r in results:\n",
    "                if r.get('code_error'): continue\n",
    "                \n",
    "                # individual cases\n",
    "                for result, _ in r['test_results']:\n",
    "                    total_test_cases += 1\n",
    "                    passed_test_cases += result == 'pass'\n",
    "                \n",
    "                # perfect solutions\n",
    "                if all(res == 'pass' for res, _ in r['test_results']):\n",
    "                    perfect_solutions += 1\n",
    "        \n",
    "        pass_rates.append(passed_test_cases / total_test_cases * 100)\n",
    "        perfect_rates.append(perfect_solutions / (n_examples * len(results)) * 100)\n",
    "    \n",
    "    return pass_rates, perfect_rates\n",
    "\n",
    "props = [0.2, 0.4, 0.6, 0.8]\n",
    "model_pairs = [\n",
    "    ('outputs/1b_test_outputs.jsonl', 'outputs/8b_test_outputs.jsonl'),\n",
    "    ('outputs/8b_test_outputs.jsonl', 'outputs/70b_test_outputs.jsonl'),\n",
    "    ('outputs/1b_test_outputs.jsonl', 'outputs/70b_test_outputs.jsonl')\n",
    "]\n",
    "\n",
    "for small_file, large_file in model_pairs:\n",
    "    print(f\"\\n{small_file.split('/')[-1]} vs {large_file.split('/')[-1]}\")\n",
    "    for prop in [0., 0.2, 0.4, 0.6, 0.8, 1.]:\n",
    "        pass_rates, perfect_rates = evaluate_routing(small_file, large_file, prop)\n",
    "        print(f\"\\n{prop*100}% routed to small model:\")\n",
    "        print(f\"Pass rate: {np.mean(pass_rates):.1f}% Â± {np.std(pass_rates):.1f}%\")\n",
    "        print(f\"Perfect rate: {np.mean(perfect_rates):.1f}% Â± {np.std(perfect_rates):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute winrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sample import format_prompt\n",
    "\n",
    "def compare(gen1, gen2, tok1, tok2):\n",
    "    if not gen1.get('code_error') and not gen2.get('code_error'):\n",
    "        if gen1['pass'] != gen2['pass']:\n",
    "            return gen1['pass'] > gen2['pass']\n",
    "        return tok1 <= tok2\n",
    "    if gen1.get('code_error') is None and gen2.get('code_error') is None:\n",
    "        raise Exception('both failed')\n",
    "    return not gen1.get('code_error')\n",
    "\n",
    "file1 = 'outputs/1b_train_outputs.jsonl'\n",
    "file2 = 'outputs/70b_train_outputs.jsonl'\n",
    "output = 'outputs/1b_70b_wins.jsonl'\n",
    "\n",
    "win_freq = defaultdict(int)\n",
    "with open(file1) as f1, open(file2) as f2, open(output, 'w') as f_out:\n",
    "    for line1, line2 in zip(f1, f2):\n",
    "        o1, o2 = json.loads(line1), json.loads(line2)\n",
    "        prompt = format_prompt(o1['item'])\n",
    "        wins = sum(compare(*pair) for pair in zip(o1['results'], o2['results'], o1['num_tokens'], o2['num_tokens']))\n",
    "        win_freq[wins] += 1\n",
    "        json.dump({'prompt': prompt, 'target': wins / 10}, f_out)\n",
    "        f_out.write('\\n')\n",
    "\n",
    "k, v = zip(*list(sorted(win_freq.items(), key=lambda x: x[0])))\n",
    "plt.bar(list(k), list(v))\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/1b_70b_wins.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sample import format_prompt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def analyze_model_comparison(small, large, output_file):\n",
    "    win_freq = defaultdict(int)\n",
    "    reason_freq = defaultdict(int)\n",
    "    \n",
    "    def get_win_reason(gen1, gen2, tok1, tok2):\n",
    "        if not gen1.get('code_error') and not gen2.get('code_error'):\n",
    "            if gen1['pass'] != gen2['pass']:\n",
    "                return 'test_cases', gen1['pass'] > gen2['pass']\n",
    "            return 'tokens', tok1 <= tok2\n",
    "        if gen1.get('code_error') is None and gen2.get('code_error') is None:\n",
    "            raise Exception('both failed')\n",
    "        return 'code_error', not gen1.get('code_error')\n",
    "\n",
    "    with open(small) as f1, open(large) as f2, open(output_file, 'w') as f_out:\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            o1, o2 = json.loads(line1), json.loads(line2)\n",
    "            prompt = format_prompt(o1['item'])\n",
    "            \n",
    "            wins = 0\n",
    "            for pair in zip(o1['results'], o2['results'], o1['num_tokens'], o2['num_tokens']):\n",
    "                reason, is_win = get_win_reason(*pair)\n",
    "                wins += is_win\n",
    "                reason_freq[reason] += 1\n",
    "                \n",
    "            win_freq[wins] += 1\n",
    "            json.dump({'prompt': prompt, 'target': wins / 10}, f_out)\n",
    "            f_out.write('\\n')\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    gs = GridSpec(1, 2, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    bar_color = \"#4878D0\"\n",
    "    edge_color = \"#2F4858\"\n",
    "    \n",
    "    k, v = zip(*sorted(win_freq.items(), key=lambda x: x[0]))\n",
    "    sns.barplot(x=list(k), y=list(v), ax=ax1, color=bar_color, edgecolor=edge_color)\n",
    "    ax1.set_title('Distribution of Model Wins', fontsize=11, pad=15)\n",
    "    ax1.set_xlabel('Number of Wins (out of 10)', fontsize=10)\n",
    "    ax1.set_ylabel('Frequency', fontsize=10)\n",
    "    ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "\n",
    "    reasons, counts = zip(*sorted(reason_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "    reason_labels = {'tokens': 'Less Tokens', 'test_cases': 'More Test Cases', 'code_error': 'Code Error'}\n",
    "    formatted_reasons = [reason_labels[r] for r in reasons]\n",
    "\n",
    "    sns.barplot(x=formatted_reasons, y=counts, ax=ax2, color=bar_color, edgecolor=edge_color)\n",
    "    ax2.set_title('Determining Factor by Frequency', fontsize=11, pad=15)\n",
    "    ax2.set_xlabel('Determining Factor', fontsize=10)\n",
    "    ax2.set_ylabel('Frequency', fontsize=10)\n",
    "    ax2.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, win_freq, reason_freq\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/1b_train_outputs.jsonl',\n",
    "    'outputs/70b_train_outputs.jsonl',\n",
    "    'outputs/1b_70b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_analysis.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/8b_train_outputs.jsonl',\n",
    "    'outputs/70b_train_outputs.jsonl',\n",
    "    'outputs/8b_70b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_analysis.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/1b_train_outputs.jsonl',\n",
    "    'outputs/8b_train_outputs.jsonl',\n",
    "    'outputs/1b_8b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_analysis.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
