{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disclosure: lots of AI-generated boilerplate code scattered throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "    \n",
    "def evaluate_routing(small_file, large_file, small_proportion, n_trials= 10, seed=42):\n",
    "    with open(small_file) as f: small_outputs = [json.loads(line) for line in f]\n",
    "    with open(large_file) as f: large_outputs = [json.loads(line) for line in f]\n",
    "        \n",
    "    n_examples = len(small_outputs)\n",
    "    n_to_small = int(small_proportion * n_examples)\n",
    "    \n",
    "    pass_rates = []\n",
    "    perfect_rates = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        small_set = set(rng.choice(n_examples, n_to_small, replace=False))\n",
    "        \n",
    "        total_test_cases = 0\n",
    "        passed_test_cases = 0\n",
    "        perfect_solutions = 0\n",
    "        \n",
    "        for i in range(n_examples):\n",
    "            results = small_outputs[i]['results'] if i in small_set else large_outputs[i]['results']\n",
    "            \n",
    "            for r in results:\n",
    "                if r.get('code_error'): continue\n",
    "                \n",
    "                # individual cases\n",
    "                for result, _ in r['test_results']:\n",
    "                    total_test_cases += 1\n",
    "                    passed_test_cases += result == 'pass'\n",
    "                \n",
    "                # perfect solutions\n",
    "                if all(res == 'pass' for res, _ in r['test_results']):\n",
    "                    perfect_solutions += 1\n",
    "        \n",
    "        pass_rates.append(passed_test_cases / total_test_cases * 100)\n",
    "        perfect_rates.append(perfect_solutions / (n_examples * len(results)) * 100)\n",
    "    \n",
    "    return pass_rates, perfect_rates\n",
    "\n",
    "props = [0.2, 0.4, 0.6, 0.8]\n",
    "model_pairs = [\n",
    "    ('outputs/1b_test_outputs.jsonl', 'outputs/8b_test_outputs.jsonl'),\n",
    "    ('outputs/8b_test_outputs.jsonl', 'outputs/70b_test_outputs.jsonl'),\n",
    "    ('outputs/1b_test_outputs.jsonl', 'outputs/70b_test_outputs.jsonl')\n",
    "]\n",
    "\n",
    "for small_file, large_file in model_pairs:\n",
    "    print(f\"\\n{small_file.split('/')[-1]} vs {large_file.split('/')[-1]}\")\n",
    "    for prop in [0., 0.2, 0.4, 0.6, 0.8, 1.]:\n",
    "        pass_rates, perfect_rates = evaluate_routing(small_file, large_file, prop)\n",
    "        print(f\"\\n{prop*100}% routed to small model:\")\n",
    "        print(f\"Pass rate: {np.mean(pass_rates):.1f}% Â± {np.std(pass_rates):.1f}%\")\n",
    "        print(f\"Perfect rate: {np.mean(perfect_rates):.1f}% Â± {np.std(perfect_rates):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute winrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sample import format_prompt\n",
    "\n",
    "def compare(gen1, gen2, tok1, tok2):\n",
    "    if not gen1.get('code_error') and not gen2.get('code_error'):\n",
    "        if gen1['pass'] != gen2['pass']:\n",
    "            return gen1['pass'] > gen2['pass']\n",
    "        return tok1 <= tok2\n",
    "    if gen1.get('code_error') is None and gen2.get('code_error') is None:\n",
    "        raise Exception('both failed')\n",
    "    return not gen1.get('code_error')\n",
    "\n",
    "file1 = 'outputs/1b_train_outputs.jsonl'\n",
    "file2 = 'outputs/70b_train_outputs.jsonl'\n",
    "output = 'outputs/1b_70b_wins.jsonl'\n",
    "\n",
    "win_freq = defaultdict(int)\n",
    "with open(file1) as f1, open(file2) as f2, open(output, 'w') as f_out:\n",
    "    for line1, line2 in zip(f1, f2):\n",
    "        o1, o2 = json.loads(line1), json.loads(line2)\n",
    "        prompt = format_prompt(o1['item'])\n",
    "        wins = sum(compare(*pair) for pair in zip(o1['results'], o2['results'], o1['num_tokens'], o2['num_tokens']))\n",
    "        win_freq[wins] += 1\n",
    "        json.dump({'prompt': prompt, 'target': wins / 10}, f_out)\n",
    "        f_out.write('\\n')\n",
    "\n",
    "k, v = zip(*list(sorted(win_freq.items(), key=lambda x: x[0])))\n",
    "plt.bar(list(k), list(v))\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/1b_70b_wins.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sample import format_prompt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def analyze_model_comparison(small, large, output_file):\n",
    "    win_freq = defaultdict(int)\n",
    "    reason_freq = defaultdict(int)\n",
    "    \n",
    "    def get_win_reason(gen1, gen2, tok1, tok2):\n",
    "        if not gen1.get('code_error') and not gen2.get('code_error'):\n",
    "            if gen1['pass'] != gen2['pass']:\n",
    "                return 'test_cases', gen1['pass'] > gen2['pass']\n",
    "            return 'tokens', tok1 <= tok2\n",
    "        if gen1.get('code_error') is None and gen2.get('code_error') is None:\n",
    "            raise Exception('both failed')\n",
    "        return 'code_error', not gen1.get('code_error')\n",
    "\n",
    "    with open(small) as f1, open(large) as f2, open(output_file, 'w') as f_out:\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            o1, o2 = json.loads(line1), json.loads(line2)\n",
    "            prompt = format_prompt(o1['item'])\n",
    "            \n",
    "            wins = 0\n",
    "            for pair in zip(o1['results'], o2['results'], o1['num_tokens'], o2['num_tokens']):\n",
    "                reason, is_win = get_win_reason(*pair)\n",
    "                wins += is_win\n",
    "                reason_freq[reason] += 1\n",
    "                \n",
    "            win_freq[wins] += 1\n",
    "            json.dump({'prompt': prompt, 'target': wins / 10}, f_out)\n",
    "            f_out.write('\\n')\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    gs = GridSpec(1, 2, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    bar_color = \"#4878D0\"\n",
    "    edge_color = \"#2F4858\"\n",
    "    \n",
    "    k, v = zip(*sorted(win_freq.items(), key=lambda x: x[0]))\n",
    "    sns.barplot(x=list(k), y=list(v), ax=ax1, color=bar_color, edgecolor=edge_color)\n",
    "    ax1.set_title('Distribution of Model Wins', fontsize=11, pad=15)\n",
    "    ax1.set_xlabel('Number of Wins (out of 10)', fontsize=10)\n",
    "    ax1.set_ylabel('Frequency', fontsize=10)\n",
    "    ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "\n",
    "    reasons, counts = zip(*sorted(reason_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "    reason_labels = {'tokens': 'Less Tokens', 'test_cases': 'More Test Cases', 'code_error': 'Code Error'}\n",
    "    formatted_reasons = [reason_labels[r] for r in reasons]\n",
    "\n",
    "    sns.barplot(x=formatted_reasons, y=counts, ax=ax2, color=bar_color, edgecolor=edge_color)\n",
    "    ax2.set_title('Determining Factor by Frequency', fontsize=11, pad=15)\n",
    "    ax2.set_xlabel('Determining Factor', fontsize=10)\n",
    "    ax2.set_ylabel('Frequency', fontsize=10)\n",
    "    ax2.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, win_freq, reason_freq\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/1b_train_outputs.jsonl',\n",
    "    'outputs/70b_train_outputs.jsonl',\n",
    "    'outputs/1b_70b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_analysis.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/8b_train_outputs.jsonl',\n",
    "    'outputs/70b_train_outputs.jsonl',\n",
    "    'outputs/8b_70b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_analysis.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/1b_train_outputs.jsonl',\n",
    "    'outputs/8b_train_outputs.jsonl',\n",
    "    'outputs/1b_8b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold tuning and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from generate import format_prompt\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('google-research-datasets/mbpp')\n",
    "\n",
    "with open('data/test_prompts.jsonl', 'w') as f:\n",
    "    for item in dataset['test']:\n",
    "        json.dump({'prompt': format_prompt(item), 'target': 0}, f)\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open('data/val_prompts.jsonl', 'w') as f:\n",
    "    for item in dataset['validation']:\n",
    "        json.dump({'prompt': format_prompt(item), 'target': 0}, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from router import Router, RouterDataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "def load_model(from_checkpoint):\n",
    "    model = Router(hidden_size=384)\n",
    "    weights = torch.load(from_checkpoint, map_location='cpu')['ema']\n",
    "\n",
    "    weight_map = {\n",
    "        'ema_model.head.0.weight': model.head[0].weight,\n",
    "        'ema_model.head.0.bias': model.head[0].bias,\n",
    "        'ema_model.head.3.weight': model.head[3].weight,\n",
    "        'ema_model.head.3.bias': model.head[3].bias\n",
    "    }\n",
    "\n",
    "    for ema_key, model_param in weight_map.items():\n",
    "        model_param.data.copy_(weights[ema_key])\n",
    "    \n",
    "    return model.eval().to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_preds(from_checkpoint, split):\n",
    "    dataset = RouterDataset(input=f'data/{split}_prompts.jsonl')\n",
    "    loader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    model = load_model(from_checkpoint)\n",
    "    all_preds = []\n",
    "    for batch in tqdm(loader):\n",
    "        logits = model.forward(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "        preds = torch.sigmoid(logits)\n",
    "        all_preds.extend(preds.tolist())\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_thresholds(from_checkpoint, small_outputs, large_outputs, title=''):\n",
    "    preds = np.array(get_preds(from_checkpoint, split='val'))\n",
    "\n",
    "    with open(small_outputs, 'r') as f:\n",
    "        small_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "    with open(large_outputs, 'r') as f:\n",
    "        large_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "        \n",
    "    large_pass = np.sum(large_results) / (3 * len(large_results))\n",
    "    \n",
    "    percentiles = [20, 40, 60, 80]\n",
    "    percentile_thresholds = np.percentile(preds, percentiles)\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    hist_color = \"#4878D0\"\n",
    "    threshold_color = \"#2F4858\"\n",
    "    plt.hist(preds, bins=30, color=hist_color, alpha=0.6, edgecolor='white')\n",
    "    for t in percentile_thresholds: plt.axvline(x=t, color=threshold_color, linestyle='--', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Model Confidence Score', fontsize=10)\n",
    "    plt.ylabel('Frequency', fontsize=10)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    for spine in ['top', 'right']: plt.gca().spines[spine].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for p, t in zip(percentiles, percentile_thresholds):\n",
    "        route_small = preds < t\n",
    "        mixed_pass = np.sum(np.where(route_small, small_results, large_results)) / (3 * len(large_results))\n",
    "        cost_adv = np.sum(route_small) / len(preds)\n",
    "        perf_gap = mixed_pass - large_pass\n",
    "        print(f\"{p}th percentile:\")\n",
    "        print(f\"  t: {t:.3f}\")\n",
    "        print(f\"  perf_gap: {perf_gap:.3f}\")\n",
    "        print(f\"  cost_adv: {cost_adv:.3f}\")\n",
    "        \n",
    "    return plt.gcf(), preds\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/8b_70b_aug=3_epoch=10.pt',\n",
    "    'data/8b_val_outputs.jsonl',\n",
    "    'data/70b_val_outputs.jsonl',\n",
    "    title='8B vs. 70B'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_threshold.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/1b_8b_aug=3_epoch=10.pt', \n",
    "    'data/1b_val_outputs.jsonl',\n",
    "    'data/8b_val_outputs.jsonl',\n",
    "    title='1B vs. 8B'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_threshold.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/1b_70b_aug=3_epoch=10.pt', \n",
    "    'data/1b_val_outputs.jsonl',\n",
    "    'data/70b_val_outputs.jsonl',\n",
    "    title='1B vs. 70B'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_threshold.png', bbox_inches='tight', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
