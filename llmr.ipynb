{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disclosure: lots of AI-generated boilerplate code scattered throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "    \n",
    "def evaluate_routing(small_file, large_file, small_proportion, n_trials=10, seed=42):\n",
    "    with open(small_file) as f: small_outputs = [json.loads(line) for line in f]\n",
    "    with open(large_file) as f: large_outputs = [json.loads(line) for line in f]\n",
    "        \n",
    "    n_examples = len(small_outputs)\n",
    "    n_to_small = int(small_proportion * n_examples)\n",
    "    \n",
    "    pass_rates = []\n",
    "    perfect_rates = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        small_set = set(rng.choice(n_examples, n_to_small, replace=False))\n",
    "        \n",
    "        total_test_cases = 0\n",
    "        passed_test_cases = 0\n",
    "        perfect_solutions = 0\n",
    "        \n",
    "        for i in range(n_examples):\n",
    "            results = small_outputs[i]['results'] if i in small_set else large_outputs[i]['results']\n",
    "            \n",
    "            for r in results:\n",
    "                if r.get('code_error'): continue\n",
    "                \n",
    "                # individual cases\n",
    "                for result, _ in r['test_results']:\n",
    "                    total_test_cases += 1\n",
    "                    passed_test_cases += result == 'pass'\n",
    "                \n",
    "                # perfect solutions\n",
    "                if all(res == 'pass' for res, _ in r['test_results']):\n",
    "                    perfect_solutions += 1\n",
    "        \n",
    "        pass_rates.append(passed_test_cases / total_test_cases * 100)\n",
    "        perfect_rates.append(perfect_solutions / (n_examples * len(results)) * 100)\n",
    "    \n",
    "    return pass_rates, perfect_rates\n",
    "\n",
    "props = [0.2, 0.4, 0.6, 0.8]\n",
    "model_pairs = [\n",
    "    ('outputs/1b_test_outputs.jsonl', 'outputs/8b_test_outputs.jsonl'),\n",
    "    ('outputs/8b_test_outputs.jsonl', 'outputs/70b_test_outputs.jsonl'),\n",
    "    ('outputs/1b_test_outputs.jsonl', 'outputs/70b_test_outputs.jsonl')\n",
    "]\n",
    "\n",
    "for small_file, large_file in model_pairs:\n",
    "    print(f\"\\n{small_file.split('/')[-1]} vs {large_file.split('/')[-1]}\")\n",
    "    for prop in [0., 0.2, 0.4, 0.6, 0.8, 1.]:\n",
    "        pass_rates, perfect_rates = evaluate_routing(small_file, large_file, prop)\n",
    "        print(f\"\\n{prop*100}% routed to small model:\")\n",
    "        print(f\"Pass rate: {np.mean(pass_rates):.1f}% ± {np.std(pass_rates):.1f}%\")\n",
    "        print(f\"Perfect rate: {np.mean(perfect_rates):.1f}% ± {np.std(perfect_rates):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute soft labels from paired data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from generate import format_prompt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def analyze_model_comparison(small, large, output_file):\n",
    "    win_freq = defaultdict(int)\n",
    "    reason_freq = defaultdict(int)\n",
    "\n",
    "    def get_win_brevity(gen1, gen2, tok1, tok2):\n",
    "        if not gen1.get('code_error') and not gen2.get('code_error'):\n",
    "            if gen1['pass'] != gen2['pass']:\n",
    "                return 'test_cases', gen1['pass'] > gen2['pass']\n",
    "            return 'tokens', tok1 <= tok2\n",
    "        if gen1.get('code_error') is None and gen2.get('code_error') is None:\n",
    "            raise Exception('both failed')\n",
    "        return 'code_error', not gen1.get('code_error')\n",
    "    \n",
    "    def get_win_standard(gen1, gen2, tok1, tok2):\n",
    "        return 'test_cases', gen1['pass'] > gen2['pass']\n",
    "\n",
    "    with open(small) as f1, open(large) as f2, open(output_file, 'w') as f_out:\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            o1, o2 = json.loads(line1), json.loads(line2)\n",
    "            prompt = format_prompt(o1['item'])\n",
    "            \n",
    "            wins = 0\n",
    "            for pair in zip(o1['results'], o2['results'], o1['num_tokens'], o2['num_tokens']):\n",
    "                reason, is_win = get_win_standard(*pair)\n",
    "                wins += is_win\n",
    "                reason_freq[reason] += 1\n",
    "                \n",
    "            win_freq[wins] += 1\n",
    "            json.dump({'prompt': prompt, 'target': wins / 10}, f_out)\n",
    "            f_out.write('\\n')\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    gs = GridSpec(1, 1, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    bar_color = \"#4878D0\"\n",
    "    edge_color = \"#2F4858\"\n",
    "    \n",
    "    k, v = zip(*sorted(win_freq.items(), key=lambda x: x[0]))\n",
    "    sns.barplot(x=list(k), y=list(v), ax=ax1, color=bar_color, edgecolor=edge_color)\n",
    "    ax1.set_xlabel('Number of Wins (out of 10)', fontsize=10)\n",
    "    ax1.set_ylabel('Frequency', fontsize=10)\n",
    "    ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, win_freq, reason_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with brevity quality function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/1b_train_outputs.jsonl',\n",
    "    'outputs/70b_train_outputs.jsonl',\n",
    "    'outputs/1b_70b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_analysis.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/8b_train_outputs.jsonl',\n",
    "    'outputs/70b_train_outputs.jsonl',\n",
    "    'outputs/8b_70b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_analysis.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/1b_train_outputs.jsonl',\n",
    "    'outputs/8b_train_outputs.jsonl',\n",
    "    'outputs/1b_8b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with standard quality function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'data/1b_train_outputs.jsonl',\n",
    "    'data/70b_train_outputs.jsonl',\n",
    "    'data/1b_70b_wins_simpl.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_analysis_simpl.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'data/8b_train_outputs.jsonl',\n",
    "    'data/70b_train_outputs.jsonl',\n",
    "    'data/8b_70b_wins_simpl.jsonl'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_analysis_simpl.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'data/1b_train_outputs.jsonl',\n",
    "    'data/8b_train_outputs.jsonl',\n",
    "    'data/1b_8b_wins_simpl.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_analysis_simpl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute winrates (with data augmentation)\n",
    "\n",
    "challenge: our scores aren't continuous and it's hard to grid search over the discrete space we've created\n",
    "\n",
    "solution: introduce a heuristic code quality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(gen, num_tokens, max_tokens=512):\n",
    "    if gen.get('code_error'): return 0.0\n",
    "    test_score = gen['pass'] / 3\n",
    "    token_score = 1 - (num_tokens / max_tokens)\n",
    "    return (0.8 * test_score) + (0.2 * token_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from generate import format_prompt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from collections import Counter\n",
    "\n",
    "def find_optimal_t(quality_gaps_by_prompt):\n",
    "    t_values = np.linspace(0, 1, 100)\n",
    "    \n",
    "    def diversity_score(t):\n",
    "        prob_labels = [np.mean([gap >= -t for gap in gaps]) for gaps in quality_gaps_by_prompt]\n",
    "        return sum(abs(l1 - l2) for i, l1 in enumerate(prob_labels) for _, l2 in enumerate(prob_labels[i+1:]))\n",
    "    \n",
    "    scores = [diversity_score(t) for t in t_values]\n",
    "    optimal_t = t_values[np.argmax(scores)]\n",
    "    return optimal_t\n",
    "\n",
    "def analyze_model_comparison(small, large, output_file, title='', augmented=False):\n",
    "    quality_gaps = []\n",
    "    quality_scores_small = []\n",
    "    quality_scores_large = []\n",
    "    wins_per_prompt = []\n",
    "    quality_gaps_by_prompt = []\n",
    "\n",
    "    bar_color = \"#4878D0\"\n",
    "    edge_color = \"#2F4858\"\n",
    "    \n",
    "    with open(small) as f1, open(large) as f2, open(output_file, 'w') as f_out:\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            o1, o2 = json.loads(line1), json.loads(line2)\n",
    "            prompt = format_prompt(o1['item'])\n",
    "            \n",
    "            wins = 0\n",
    "            prompt_gaps = []\n",
    "            for gen1, gen2, tok1, tok2 in zip(o1['results'], o2['results'], o1['num_tokens'], o2['num_tokens']):\n",
    "                score1 = quality(gen1, tok1)\n",
    "                score2 = quality(gen2, tok2)\n",
    "                quality_gap = score1 - score2\n",
    "                quality_scores_small.append(score1)\n",
    "                quality_scores_large.append(score2)\n",
    "                quality_gaps.append(quality_gap)\n",
    "                wins += (score1 >= score2)\n",
    "                prompt_gaps.append(score1 - score2)\n",
    "            \n",
    "            quality_gaps_by_prompt.append(prompt_gaps)\n",
    "            wins_per_prompt.append(wins)\n",
    "\n",
    "            json.dump({'prompt': prompt, 'target': wins / len(o1['results'])}, f_out)\n",
    "            f_out.write('\\n')\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    gs = GridSpec(1, 1, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    if not augmented:\n",
    "        fig.suptitle(title, fontsize=12)\n",
    "        win_freq = Counter(wins_per_prompt)\n",
    "        k, v = zip(*sorted(win_freq.items(), key=lambda x: x[0]))\n",
    "    else:\n",
    "        fig.suptitle(title+', augmented', fontsize=12)\n",
    "        optimal_t = find_optimal_t(quality_gaps_by_prompt)\n",
    "        augmented_labels = [np.mean([gap >= -optimal_t for gap in gaps]) for gaps in quality_gaps_by_prompt]\n",
    "        win_freq = Counter(10 * augmented_labels)\n",
    "\n",
    "        with open(small) as f, open(output_file, 'w') as f_out:\n",
    "            for line, aug_label in zip(f, augmented_labels):\n",
    "                prompt = format_prompt(json.loads(line)['item'])\n",
    "                json.dump({'prompt': prompt, 'target': aug_label}, f_out)\n",
    "                f_out.write('\\n')\n",
    "\n",
    "    k, v = zip(*sorted(win_freq.items(), key=lambda x: x[0]))\n",
    "    sns.barplot(x=list(k), y=list(v), ax=ax1, color=bar_color, edgecolor=edge_color)\n",
    "    ax1.set_xlabel('Number of Wins (out of 10)', fontsize=10)\n",
    "    ax1.set_ylabel('Frequency', fontsize=10)\n",
    "    ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "for pair, title in zip([\n",
    "    ('1b_train_outputs.jsonl', '8b_train_outputs.jsonl', '1b_8b'),\n",
    "    ('8b_train_outputs.jsonl', '70b_train_outputs.jsonl', '8b_70b'),\n",
    "    ('1b_train_outputs.jsonl', '70b_train_outputs.jsonl', '1b_70b'),\n",
    "], ['1B vs. 8B', '8B vs. 70B', '1B vs. 70B']):\n",
    "    small_file, large_file, name = pair\n",
    "    fig = analyze_model_comparison(f'data/{small_file}', f'data/{large_file}', f'data/{name}_quality.jsonl', title=title)\n",
    "    plt.savefig(f'figures/{name}_quality.png')\n",
    "\n",
    "for pair, title in zip(\n",
    "    [('1b_train_outputs.jsonl', '8b_train_outputs.jsonl', '1b_8b'),\n",
    "    ('8b_train_outputs.jsonl', '70b_train_outputs.jsonl', '8b_70b'),\n",
    "    ('1b_train_outputs.jsonl', '70b_train_outputs.jsonl', '1b_70b')],\n",
    "    ['1B vs. 8B', '8B vs. 70B', '1B vs. 70B']\n",
    "):\n",
    "    small_file, large_file, name = pair\n",
    "    fig = analyze_model_comparison(\n",
    "        f'data/{small_file}',\n",
    "        f'data/{large_file}',\n",
    "        f'data/{name}_quality_augmented.jsonl',\n",
    "        title=title,\n",
    "        augmented=True\n",
    "    )\n",
    "    plt.savefig(f'figures/{name}_quality_augmented.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation set threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from router import Router, RouterDataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "def load_model(from_checkpoint):\n",
    "    model = Router(hidden_size=384)\n",
    "    weights = torch.load(from_checkpoint, map_location='cpu')['ema']\n",
    "\n",
    "    weight_map = {\n",
    "        'ema_model.head.0.weight': model.head[0].weight,\n",
    "        'ema_model.head.0.bias': model.head[0].bias,\n",
    "        'ema_model.head.3.weight': model.head[3].weight,\n",
    "        'ema_model.head.3.bias': model.head[3].bias\n",
    "    }\n",
    "\n",
    "    for ema_key, model_param in weight_map.items():\n",
    "        model_param.data.copy_(weights[ema_key])\n",
    "    \n",
    "    return model.eval().to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_preds(from_checkpoint, split):\n",
    "    dataset = RouterDataset(input=f'data/{split}_prompts.jsonl')\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = load_model(from_checkpoint)\n",
    "    all_preds = []\n",
    "    for batch in tqdm(loader):\n",
    "        logits = model.forward(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "        preds = torch.sigmoid(logits)\n",
    "        all_preds.extend(preds.tolist())\n",
    "    return all_preds\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_thresholds(from_checkpoint, small_outputs, large_outputs, title=''):\n",
    "    preds = np.array(get_preds(from_checkpoint, split='val'))\n",
    "\n",
    "    with open(small_outputs, 'r') as f:\n",
    "        small_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "    with open(large_outputs, 'r') as f:\n",
    "        large_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "        \n",
    "    large_pass = np.sum(large_results) / (3 * len(large_results))\n",
    "    \n",
    "    percentiles = [20, 40, 60, 80]\n",
    "    percentile_thresholds = np.percentile(preds, percentiles)\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    hist_color = \"#4878D0\"\n",
    "    threshold_color = \"#2F4858\"\n",
    "    plt.hist(preds, bins=30, color=hist_color, alpha=0.6, edgecolor='white')\n",
    "    for t in percentile_thresholds: plt.axvline(x=t, color=threshold_color, linestyle='--', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Model Confidence Score', fontsize=10)\n",
    "    plt.ylabel('Frequency', fontsize=10)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    for spine in ['top', 'right']: plt.gca().spines[spine].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for p, t in zip(percentiles, percentile_thresholds):\n",
    "        route_small = preds < t\n",
    "        mixed_pass = np.sum(np.where(route_small, small_results, large_results)) / (3 * len(large_results))\n",
    "        cost_adv = np.sum(route_small) / len(preds)\n",
    "        perf_gap = mixed_pass - large_pass\n",
    "        print(f\"{p}th percentile:\")\n",
    "        print(f\"  t: {t:.6f}\")\n",
    "        print(f\"  perf_gap: {perf_gap:.3f}\")\n",
    "        print(f\"  cost_adv: {cost_adv:.3f}\")\n",
    "        \n",
    "    return plt.gcf(), preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with brevity quality function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/1b_8b_aug=3_epoch=10.pt', \n",
    "    'data/1b_val_outputs.jsonl',\n",
    "    'data/8b_val_outputs.jsonl',\n",
    "    title='1B vs. 8B'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_threshold.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/8b_70b_aug=3_epoch=10.pt',\n",
    "    'data/8b_val_outputs.jsonl',\n",
    "    'data/70b_val_outputs.jsonl',\n",
    "    title='8B vs. 70B'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_threshold.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/1b_70b_aug=3_epoch=10.pt', \n",
    "    'data/1b_val_outputs.jsonl',\n",
    "    'data/70b_val_outputs.jsonl',\n",
    "    title='1B vs. 70B'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_threshold.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/1b_8b_simpl_epoch=10.pt', \n",
    "    'data/1b_val_outputs.jsonl',\n",
    "    'data/8b_val_outputs.jsonl',\n",
    "    title='1B vs. 8B'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_threshold_simpl.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/8b_70b_simpl_epoch=10.pt',\n",
    "    'data/8b_val_outputs.jsonl',\n",
    "    'data/70b_val_outputs.jsonl',\n",
    "    title='8B vs. 70B'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_threshold_simpl.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/1b_70b_simpl_epoch=10.pt', \n",
    "    'data/1b_val_outputs.jsonl',\n",
    "    'data/70b_val_outputs.jsonl',\n",
    "    title='1B vs. 70B'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_threshold_simpl.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation-tuned thresholds\n",
    "\n",
    "with brevity quality function\n",
    "\n",
    "| Pair      | 20%      | 40%      | 60%      | 80%       |\n",
    "| --------- | -------- | -------- | -------- | --------- |\n",
    "| 1B vs 8B  | 0.0826   | 0.086494 | 0.089773 | 0.096273  |\n",
    "| 8B vs 70B | 0.745153 | 0.758044 | 0.766146 | 0.7747814 |\n",
    "| 1B vs 70B | 0.218676 | 0.219519 | 0.220657 | 0.222232  |\n",
    "\n",
    "with standard quality fuunction\n",
    "\n",
    "| Pair      | 20%      | 40%      | 60%      | 80%      |\n",
    "| --------- | -------- | -------- | -------- | -------- |\n",
    "| 1B vs 8B  | 0.657548 | 0.662825 | 0.667725 | 0.671890 |\n",
    "| 8B vs 70B | 0.877196 | 0.887329 | 0.893095 | 0.899906 |\n",
    "| 1B vs 70B | 0.642317 | 0.667306 | 0.681851 | 0.699679 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test set eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(from_checkpoint, small_outputs, large_outputs, thresholds):\n",
    "    preds = np.array(get_preds(from_checkpoint, split='test'))\n",
    "\n",
    "    with open(small_outputs, 'r') as f:\n",
    "        small_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "    with open(large_outputs, 'r') as f:\n",
    "        large_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "        \n",
    "    large_pass = np.sum(large_results) / (3 * len(large_results))\n",
    "    \n",
    "    for t in thresholds:\n",
    "        route_small = preds < t\n",
    "        mixed_pass = np.sum(np.where(route_small, small_results, large_results)) / (3 * len(large_results))\n",
    "        cost_adv = np.sum(route_small) / len(preds)\n",
    "        perf_gap = mixed_pass - large_pass\n",
    "        print(f\"with threshold {t}\")\n",
    "        print(f\"  t: {t:.3f}\")\n",
    "        print(f\"  perf_gap: {100*perf_gap:.1f}\")\n",
    "        print(f\"  cost_adv: {cost_adv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval with brevity quality function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(\n",
    "    'checkpoints/1b_8b_aug=3_epoch=10.pt',\n",
    "    'data/1b_test_outputs.jsonl',\n",
    "    'data/8b_test_outputs.jsonl',\n",
    "    [0.0826, 0.086494, 0.089773, 0.096273]\n",
    ")\n",
    "\n",
    "eval(\n",
    "    'checkpoints/8b_70b_aug=3_epoch=10.pt',\n",
    "    'data/8b_test_outputs.jsonl',\n",
    "    'data/70b_test_outputs.jsonl',\n",
    "    [0.745153, 0.758044, 0.766146, 0.7747814]\n",
    ")\n",
    "\n",
    "eval(\n",
    "    'checkpoints/1b_70b_aug=3_epoch=10.pt', \n",
    "    'data/1b_test_outputs.jsonl',\n",
    "    'data/70b_test_outputs.jsonl',\n",
    "    [0.218676, 0.219519, 0.220657, 0.222232]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval with standard quality function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(\n",
    "    'checkpoints/1b_8b_simpl_epoch=10.pt',\n",
    "    'data/1b_test_outputs.jsonl',\n",
    "    'data/8b_test_outputs.jsonl',\n",
    "    [0.657548, 0.662825, 0.667725, 0.671890]\n",
    ")\n",
    "\n",
    "eval(\n",
    "    'checkpoints/8b_70b_simpl_epoch=10.pt',\n",
    "    'data/8b_test_outputs.jsonl',\n",
    "    'data/70b_test_outputs.jsonl',\n",
    "    [0.877196, 0.887329, 0.893095, 0.899906]\n",
    ")\n",
    "\n",
    "eval(\n",
    "    'checkpoints/1b_70b_simpl_epoch=10.pt', \n",
    "    'data/1b_test_outputs.jsonl',\n",
    "    'data/70b_test_outputs.jsonl',\n",
    "    [0.642317, 0.667306, 0.681851, 0.699679]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set values\n",
    "\n",
    "with brevity qf\n",
    "\n",
    "| Pair      | 20%          | 40%          | 60%           | 80%           |\n",
    "| --------- | ------------ | ------------ | ------------- | ------------- |\n",
    "| 1B vs 8B  | -4.9 (0.260) | -7.9 (0.490) | -10.9 (0.596) | -15.7 (0.772) |\n",
    "| 8B vs 70B | -0.7 (0.204) | 1.5 (0.392)  | 4.9 (0.544)   | 7.3 (0.754)   |\n",
    "| 1B vs 70B | -1.7 (0.230) | -3.1 (0.372) | -6.3 (0.624)  | -11.1 (0.848) |\n",
    "\n",
    "with standard quality function\n",
    "\n",
    "| Pair      | 20%           | 40%           | 60%           | 80%           |\n",
    "| --------- | ------------- | ------------- | ------------- | ------------- |\n",
    "| 1B vs 8B  | -4.1 (0.158)  | -10.6 (0.354) | -15.3 (0.574) | -18.7 (0.782) |\n",
    "| 8B vs 70B | -0.2 (0.214)  | 0.9 (0.376)   | 3.5 (0.490)   | 6.9 (0.722)   |\n",
    "| 1B vs 70B | -5.9 (0.216)  | -9.4 (0.392)  | -10.6 (0.514) | -10.7 (0.748) |\n",
    "\n",
    "baseline\n",
    "\n",
    "| Pair      | 0%  | 20%  | 40%  | 60%   | 80%   | 100%  |\n",
    "| --------- | --- | ---- | ---- | ----- | ----- | ----- |\n",
    "| 1B vs 8B  | 0.0 | -3.1 | -7.0 | -11.1 | -14.3 | -18.5 |\n",
    "| 8B vs 70B | 0.0 | 1.0  | 1.3  | 2.3   | 3.1   | 3.9   |\n",
    "| 1B vs 70B | 0.0 | -2.4 | -6.2 | -9.3  | -11.5 | -14.6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_data(data):\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 20,\n",
    "        'axes.labelsize': 20,\n",
    "        'axes.titlesize': 20,\n",
    "        'legend.fontsize': 20,\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize':20 \n",
    "    })\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    baseline_color = '#4878D0'\n",
    "    test_color = '#EE854A'\n",
    "    \n",
    "    for ax, (pair, pair_data) in zip(axes, data.items()):\n",
    "        ax.fill_between(pair_data['baseline_x'],\n",
    "                       np.array(pair_data['baseline_y']) - np.array(pair_data['baseline_std']),\n",
    "                       np.array(pair_data['baseline_y']) + np.array(pair_data['baseline_std']),\n",
    "                       color=baseline_color, alpha=0.2)\n",
    "        \n",
    "        ax.plot(pair_data['baseline_x'], pair_data['baseline_y'], 'o-', \n",
    "               color=baseline_color, alpha=0.7, label='Baseline')\n",
    "        ax.plot(pair_data['test_x'], pair_data['test_y'], 's-', \n",
    "               color=test_color, alpha=0.9, label='Test')\n",
    "        \n",
    "        ax.set_ylabel('Performance Gap (%)') if ax.get_position().x0 < 0.1 else None\n",
    "        ax.set_title(pair)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    axes[0].legend()\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    '1B-8B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, -3.1, -7.0, -11.1, -14.3, -18.5],\n",
    "        'baseline_std': [0.0, 0.6, 0.7, 1.6, 0.9, 0.0],\n",
    "        'test_x': [0.260, 0.490, 0.596, 0.772],\n",
    "        'test_y': [-4.9, -7.9, -10.9, -15.7]\n",
    "    },\n",
    "    '8B-70B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, 1.0, 1.3, 2.3, 3.1, 3.9],\n",
    "        'baseline_std': [0.0, 0.8, 0.6, 0.6, 0.7, 0.0],\n",
    "        'test_x': [0.204, 0.392, 0.544, 0.754],\n",
    "        'test_y': [-0.7, 1.5, 4.9, 7.3]\n",
    "    },\n",
    "    '1B-70B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, -2.4, -6.2, -9.3, -11.5, -14.6],\n",
    "        'baseline_std': [0.0, 0.7, 0.9, 1.3, 1.0, 0.0],\n",
    "        'test_x': [0.230, 0.372, 0.624, 0.848],\n",
    "        'test_y': [-1.7, -3.1, -6.3, -11.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "fig = plot_all_data(data)\n",
    "plt.savefig('figures/all_perf_comparison.png', bbox_inches='tight', dpi=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    '1B-8B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, -3.1, -7.0, -11.1, -14.3, -18.5],\n",
    "        'baseline_std': [0.0, 0.6, 0.7, 1.6, 0.9, 0.0],\n",
    "        'test_x': [0.158, 0.354, 0.574, 0.782],\n",
    "        'test_y': [-4.1, -10.6, -15.3, -18.7]\n",
    "    },\n",
    "    '8B-70B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, 1.0, 1.3, 2.3, 3.1, 3.9],\n",
    "        'baseline_std': [0.0, 0.8, 0.6, 0.6, 0.7, 0.0],\n",
    "        'test_x': [0.214, 0.376, 0.490, 0.722],\n",
    "        'test_y': [-0.2, 0.9, 3.5, 6.9]\n",
    "    },\n",
    "    '1B-70B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, -2.4, -6.2, -9.3, -11.5, -14.6],\n",
    "        'baseline_std': [0.0, 0.7, 0.9, 1.3, 1.0, 0.0],\n",
    "        'test_x': [0.216, 0.392, 0.514, 0.748],\n",
    "        'test_y': [-5.9, -9.4, -10.6, -10.7]\n",
    "    }\n",
    "}\n",
    "\n",
    "fig = plot_all_data(data)\n",
    "plt.savefig('figures/all_perf_comparison_simpl.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just a plot of the winrates for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def extract_winrates(filename):\n",
    "    winrates = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            winrates.append(data['target'])\n",
    "    return winrates\n",
    "\n",
    "def plot_model_winrates(file1, file2, file3, output_file):\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 20,\n",
    "        'axes.labelsize': 20,\n",
    "        'axes.titlesize': 20,\n",
    "        'legend.fontsize': 20,\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize':20 \n",
    "    })\n",
    "    \n",
    "    winrates1 = extract_winrates(file1)\n",
    "    winrates2 = extract_winrates(file2)\n",
    "    winrates3 = extract_winrates(file3)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    sns.kdeplot(data=winrates1, label='1B-70B', ax=ax)\n",
    "    sns.kdeplot(data=winrates2, label='8B-70B', ax=ax)\n",
    "    sns.kdeplot(data=winrates3, label='1B-8B', ax=ax)\n",
    "    \n",
    "    ax.set_ylabel('', fontsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    return fig\n",
    "\n",
    "files = [\n",
    "    'data/1b_70b_wins_simpl.jsonl',\n",
    "    'data/8b_70b_wins_simpl.jsonl',\n",
    "    'data/1b_8b_wins_simpl.jsonl'\n",
    "]\n",
    "\n",
    "_ = plot_model_winrates(\n",
    "    files[0],\n",
    "    files[1],\n",
    "    files[2],\n",
    "    'figures/winrates_simpl.png'\n",
    ")\n",
    "\n",
    "files = [\n",
    "    'data/1b_70b_wins.jsonl',\n",
    "    'data/8b_70b_wins.jsonl',\n",
    "    'data/1b_8b_wins.jsonl'\n",
    "]\n",
    "\n",
    "_ = plot_model_winrates(\n",
    "    files[0],\n",
    "    files[1],\n",
    "    files[2],\n",
    "    'figures/winrates.png'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
