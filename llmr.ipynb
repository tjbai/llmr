{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disclosure: lots of AI-generated boilerplate code scattered throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "    \n",
    "def evaluate_routing(small_file, large_file, small_proportion, n_trials= 10, seed=42):\n",
    "    with open(small_file) as f: small_outputs = [json.loads(line) for line in f]\n",
    "    with open(large_file) as f: large_outputs = [json.loads(line) for line in f]\n",
    "        \n",
    "    n_examples = len(small_outputs)\n",
    "    n_to_small = int(small_proportion * n_examples)\n",
    "    \n",
    "    pass_rates = []\n",
    "    perfect_rates = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        small_set = set(rng.choice(n_examples, n_to_small, replace=False))\n",
    "        \n",
    "        total_test_cases = 0\n",
    "        passed_test_cases = 0\n",
    "        perfect_solutions = 0\n",
    "        \n",
    "        for i in range(n_examples):\n",
    "            results = small_outputs[i]['results'] if i in small_set else large_outputs[i]['results']\n",
    "            \n",
    "            for r in results:\n",
    "                if r.get('code_error'): continue\n",
    "                \n",
    "                # individual cases\n",
    "                for result, _ in r['test_results']:\n",
    "                    total_test_cases += 1\n",
    "                    passed_test_cases += result == 'pass'\n",
    "                \n",
    "                # perfect solutions\n",
    "                if all(res == 'pass' for res, _ in r['test_results']):\n",
    "                    perfect_solutions += 1\n",
    "        \n",
    "        pass_rates.append(passed_test_cases / total_test_cases * 100)\n",
    "        perfect_rates.append(perfect_solutions / (n_examples * len(results)) * 100)\n",
    "    \n",
    "    return pass_rates, perfect_rates\n",
    "\n",
    "props = [0.2, 0.4, 0.6, 0.8]\n",
    "model_pairs = [\n",
    "    ('outputs/1b_test_outputs.jsonl', 'outputs/8b_test_outputs.jsonl'),\n",
    "    ('outputs/8b_test_outputs.jsonl', 'outputs/70b_test_outputs.jsonl'),\n",
    "    ('outputs/1b_test_outputs.jsonl', 'outputs/70b_test_outputs.jsonl')\n",
    "]\n",
    "\n",
    "for small_file, large_file in model_pairs:\n",
    "    print(f\"\\n{small_file.split('/')[-1]} vs {large_file.split('/')[-1]}\")\n",
    "    for prop in [0., 0.2, 0.4, 0.6, 0.8, 1.]:\n",
    "        pass_rates, perfect_rates = evaluate_routing(small_file, large_file, prop)\n",
    "        print(f\"\\n{prop*100}% routed to small model:\")\n",
    "        print(f\"Pass rate: {np.mean(pass_rates):.1f}% Â± {np.std(pass_rates):.1f}%\")\n",
    "        print(f\"Perfect rate: {np.mean(perfect_rates):.1f}% Â± {np.std(perfect_rates):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute winrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sample import format_prompt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def analyze_model_comparison(small, large, output_file):\n",
    "    win_freq = defaultdict(int)\n",
    "    reason_freq = defaultdict(int)\n",
    "    \n",
    "    def get_win_reason(gen1, gen2, tok1, tok2):\n",
    "        if not gen1.get('code_error') and not gen2.get('code_error'):\n",
    "            if gen1['pass'] != gen2['pass']:\n",
    "                return 'test_cases', gen1['pass'] > gen2['pass']\n",
    "            return 'tokens', tok1 <= tok2\n",
    "        if gen1.get('code_error') is None and gen2.get('code_error') is None:\n",
    "            raise Exception('both failed')\n",
    "        return 'code_error', not gen1.get('code_error')\n",
    "\n",
    "    with open(small) as f1, open(large) as f2, open(output_file, 'w') as f_out:\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            o1, o2 = json.loads(line1), json.loads(line2)\n",
    "            prompt = format_prompt(o1['item'])\n",
    "            \n",
    "            wins = 0\n",
    "            for pair in zip(o1['results'], o2['results'], o1['num_tokens'], o2['num_tokens']):\n",
    "                reason, is_win = get_win_reason(*pair)\n",
    "                wins += is_win\n",
    "                reason_freq[reason] += 1\n",
    "                \n",
    "            win_freq[wins] += 1\n",
    "            json.dump({'prompt': prompt, 'target': wins / 10}, f_out)\n",
    "            f_out.write('\\n')\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    gs = GridSpec(1, 2, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    bar_color = \"#4878D0\"\n",
    "    edge_color = \"#2F4858\"\n",
    "    \n",
    "    k, v = zip(*sorted(win_freq.items(), key=lambda x: x[0]))\n",
    "    sns.barplot(x=list(k), y=list(v), ax=ax1, color=bar_color, edgecolor=edge_color)\n",
    "    ax1.set_title('Distribution of Model Wins', fontsize=11, pad=15)\n",
    "    ax1.set_xlabel('Number of Wins (out of 10)', fontsize=10)\n",
    "    ax1.set_ylabel('Frequency', fontsize=10)\n",
    "    ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "\n",
    "    reasons, counts = zip(*sorted(reason_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "    reason_labels = {'tokens': 'Less Tokens', 'test_cases': 'More Test Cases', 'code_error': 'Code Error'}\n",
    "    formatted_reasons = [reason_labels[r] for r in reasons]\n",
    "\n",
    "    sns.barplot(x=formatted_reasons, y=counts, ax=ax2, color=bar_color, edgecolor=edge_color)\n",
    "    ax2.set_title('Determining Factor by Frequency', fontsize=11, pad=15)\n",
    "    ax2.set_xlabel('Determining Factor', fontsize=10)\n",
    "    ax2.set_ylabel('Frequency', fontsize=10)\n",
    "    ax2.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, win_freq, reason_freq\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/1b_train_outputs.jsonl',\n",
    "    'outputs/70b_train_outputs.jsonl',\n",
    "    'outputs/1b_70b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_analysis.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/8b_train_outputs.jsonl',\n",
    "    'outputs/70b_train_outputs.jsonl',\n",
    "    'outputs/8b_70b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_analysis.png')\n",
    "\n",
    "fig, wins, reasons = analyze_model_comparison(\n",
    "    'outputs/1b_train_outputs.jsonl',\n",
    "    'outputs/8b_train_outputs.jsonl',\n",
    "    'outputs/1b_8b_wins.jsonl'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation set threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from generate import format_prompt\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('google-research-datasets/mbpp')\n",
    "\n",
    "with open('data/test_prompts.jsonl', 'w') as f:\n",
    "    for item in dataset['test']:\n",
    "        json.dump({'prompt': format_prompt(item), 'target': 0}, f)\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open('data/val_prompts.jsonl', 'w') as f:\n",
    "    for item in dataset['validation']:\n",
    "        json.dump({'prompt': format_prompt(item), 'target': 0}, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from router import Router, RouterDataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "def load_model(from_checkpoint):\n",
    "    model = Router(hidden_size=384)\n",
    "    weights = torch.load(from_checkpoint, map_location='cpu')['ema']\n",
    "\n",
    "    weight_map = {\n",
    "        'ema_model.head.0.weight': model.head[0].weight,\n",
    "        'ema_model.head.0.bias': model.head[0].bias,\n",
    "        'ema_model.head.3.weight': model.head[3].weight,\n",
    "        'ema_model.head.3.bias': model.head[3].bias\n",
    "    }\n",
    "\n",
    "    for ema_key, model_param in weight_map.items():\n",
    "        model_param.data.copy_(weights[ema_key])\n",
    "    \n",
    "    return model.eval().to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_preds(from_checkpoint, split):\n",
    "    dataset = RouterDataset(input=f'data/{split}_prompts.jsonl')\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = load_model(from_checkpoint)\n",
    "    all_preds = []\n",
    "    for batch in tqdm(loader):\n",
    "        logits = model.forward(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "        preds = torch.sigmoid(logits)\n",
    "        all_preds.extend(preds.tolist())\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_thresholds(from_checkpoint, small_outputs, large_outputs, title=''):\n",
    "    preds = np.array(get_preds(from_checkpoint, split='val'))\n",
    "\n",
    "    with open(small_outputs, 'r') as f:\n",
    "        small_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "    with open(large_outputs, 'r') as f:\n",
    "        large_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "        \n",
    "    large_pass = np.sum(large_results) / (3 * len(large_results))\n",
    "    \n",
    "    percentiles = [20, 40, 60, 80]\n",
    "    percentile_thresholds = np.percentile(preds, percentiles)\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    sns.set_palette(\"muted\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    hist_color = \"#4878D0\"\n",
    "    threshold_color = \"#2F4858\"\n",
    "    plt.hist(preds, bins=30, color=hist_color, alpha=0.6, edgecolor='white')\n",
    "    for t in percentile_thresholds: plt.axvline(x=t, color=threshold_color, linestyle='--', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Model Confidence Score', fontsize=10)\n",
    "    plt.ylabel('Frequency', fontsize=10)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    for spine in ['top', 'right']: plt.gca().spines[spine].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for p, t in zip(percentiles, percentile_thresholds):\n",
    "        route_small = preds < t\n",
    "        mixed_pass = np.sum(np.where(route_small, small_results, large_results)) / (3 * len(large_results))\n",
    "        cost_adv = np.sum(route_small) / len(preds)\n",
    "        perf_gap = mixed_pass - large_pass\n",
    "        print(f\"{p}th percentile:\")\n",
    "        print(f\"  t: {t:.6f}\")\n",
    "        print(f\"  perf_gap: {perf_gap:.3f}\")\n",
    "        print(f\"  cost_adv: {cost_adv:.3f}\")\n",
    "        \n",
    "    return plt.gcf(), preds\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/1b_8b_aug=3_epoch=10.pt', \n",
    "    'data/1b_val_outputs.jsonl',\n",
    "    'data/8b_val_outputs.jsonl',\n",
    "    title='1B vs. 8B'\n",
    ")\n",
    "plt.savefig('figures/1b_8b_threshold.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/8b_70b_aug=3_epoch=10.pt',\n",
    "    'data/8b_val_outputs.jsonl',\n",
    "    'data/70b_val_outputs.jsonl',\n",
    "    title='8B vs. 70B'\n",
    ")\n",
    "plt.savefig('figures/8b_70b_threshold.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "fig, preds = analyze_thresholds(\n",
    "    'checkpoints/1b_70b_aug=3_epoch=10.pt', \n",
    "    'data/1b_val_outputs.jsonl',\n",
    "    'data/70b_val_outputs.jsonl',\n",
    "    title='1B vs. 70B'\n",
    ")\n",
    "plt.savefig('figures/1b_70b_threshold.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation-tuned percentiles thresholds (label-not-augmented)\n",
    "\n",
    "| Pair      | 20%      | 40%      | 60%      | 80%       |\n",
    "| --------- | -------- | -------- | -------- | --------- |\n",
    "| 1B vs 8B  | 0.0826   | 0.086494 | 0.089773 | 0.096273  |\n",
    "| 8B vs 70B | 0.745153 | 0.758044 | 0.766146 | 0.7747814 |\n",
    "| 1B vs 70B | 0.218676 | 0.219519 | 0.220657 | 0.222232  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test set eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(from_checkpoint, small_outputs, large_outputs, thresholds):\n",
    "    preds = np.array(get_preds(from_checkpoint, split='test'))\n",
    "\n",
    "    with open(small_outputs, 'r') as f:\n",
    "        small_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "    with open(large_outputs, 'r') as f:\n",
    "        large_results = np.array([json.loads(line)['results'][0]['pass'] for line in f])\n",
    "        \n",
    "    large_pass = np.sum(large_results) / (3 * len(large_results))\n",
    "    \n",
    "    for t in thresholds:\n",
    "        route_small = preds < t\n",
    "        mixed_pass = np.sum(np.where(route_small, small_results, large_results)) / (3 * len(large_results))\n",
    "        cost_adv = np.sum(route_small) / len(preds)\n",
    "        perf_gap = mixed_pass - large_pass\n",
    "        print(f\"with threshold {t}\")\n",
    "        print(f\"  t: {t:.3f}\")\n",
    "        print(f\"  perf_gap: {perf_gap:.3f}\")\n",
    "        print(f\"  cost_adv: {cost_adv:.3f}\")\n",
    "\n",
    "eval(\n",
    "    'checkpoints/1b_8b_aug=3_epoch=10.pt',\n",
    "    'data/1b_test_outputs.jsonl',\n",
    "    'data/8b_test_outputs.jsonl',\n",
    "    [0.0826, 0.086494, 0.089773, 0.096273]\n",
    ")\n",
    "\n",
    "eval(\n",
    "    'checkpoints/8b_70b_aug=3_epoch=10.pt',\n",
    "    'data/8b_test_outputs.jsonl',\n",
    "    'data/70b_test_outputs.jsonl',\n",
    "    [0.745153, 0.758044, 0.766146, 0.7747814]\n",
    ")\n",
    "\n",
    "eval(\n",
    "    'checkpoints/1b_70b_aug=3_epoch=10.pt', \n",
    "    'data/1b_test_outputs.jsonl',\n",
    "    'data/70b_test_outputs.jsonl',\n",
    "    [0.218676, 0.219519, 0.220657, 0.222232]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set values\n",
    "\n",
    "**perf gap vs. cost advantage for each setting (parentheses indicate actual percentile)**\n",
    "\n",
    "| Pair      | 20%          | 40%          | 60%           | 80%           |\n",
    "| --------- | ------------ | ------------ | ------------- | ------------- |\n",
    "| 1B vs 8B  | -4.9 (0.260) | -7.9 (0.490) | -10.9 (0.596) | -15.7 (0.772) |\n",
    "| 8B vs 70B | -0.7 (0.204) | 1.5 (0.392)  | 4.9 (0.544)   | 7.3 (0.754)   |\n",
    "| 1B vs 70B | -1.7 (0.230) | -3.1 (0.372) | -6.3 (0.624)  | -11.1 (0.848) |\n",
    "\n",
    "# baseline\n",
    "\n",
    "**perf gap vs. cost advantage**\n",
    "\n",
    "| Pair      | 0%  | 20%  | 40%  | 60%   | 80%   | 100%  |\n",
    "| --------- | --- | ---- | ---- | ----- | ----- | ----- |\n",
    "| 1B vs 8B  | 0.0 | -3.1 | -7.0 | -11.1 | -14.3 | -18.5 |\n",
    "| 8B vs 70B | 0.0 | 1.0  | 1.3  | 2.3   | 3.1   | 3.9   |\n",
    "| 1B vs 70B | 0.0 | -2.4 | -6.2 | -9.3  | -11.5 | -14.6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_data(data):\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    \n",
    "    baseline_color = '#4878D0'\n",
    "    test_color = '#EE854A'\n",
    "    \n",
    "    for ax, (pair, pair_data) in zip(axes, data.items()):\n",
    "        # Plot baseline with std dev\n",
    "        ax.fill_between(pair_data['baseline_x'], \n",
    "                       np.array(pair_data['baseline_y']) - np.array(pair_data['baseline_std']),\n",
    "                       np.array(pair_data['baseline_y']) + np.array(pair_data['baseline_std']),\n",
    "                       color=baseline_color, alpha=0.2)\n",
    "        \n",
    "        ax.plot(pair_data['baseline_x'], pair_data['baseline_y'], 'o-', \n",
    "               color=baseline_color, alpha=0.7, label='Baseline')\n",
    "        ax.plot(pair_data['test_x'], pair_data['test_y'], 's-', \n",
    "               color=test_color, alpha=0.9, label='Test')\n",
    "        \n",
    "        ax.set_xlabel('Cost Advantage')\n",
    "        ax.set_ylabel('Performance Gap (%)') if ax.get_position().x0 < 0.1 else None\n",
    "        ax.set_title(pair)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    axes[0].legend()\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "data = {\n",
    "    '1B vs. 8B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, -3.1, -7.0, -11.1, -14.3, -18.5],\n",
    "        'baseline_std': [0.0, 0.6, 0.7, 1.6, 0.9, 0.0],\n",
    "        'test_x': [0.260, 0.490, 0.596, 0.772],\n",
    "        'test_y': [-4.9, -7.9, -10.9, -15.7]\n",
    "    },\n",
    "    '8B vs. 70B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, 1.0, 1.3, 2.3, 3.1, 3.9],\n",
    "        'baseline_std': [0.0, 0.8, 0.6, 0.6, 0.7, 0.0],\n",
    "        'test_x': [0.204, 0.392, 0.544, 0.754],\n",
    "        'test_y': [-0.7, 1.5, 4.9, 7.3]\n",
    "    },\n",
    "    '1B vs. 70B': {\n",
    "        'baseline_x': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'baseline_y': [0.0, -2.4, -6.2, -9.3, -11.5, -14.6],\n",
    "        'baseline_std': [0.0, 0.7, 0.9, 1.3, 1.0, 0.0],\n",
    "        'test_x': [0.230, 0.372, 0.624, 0.848],\n",
    "        'test_y': [-1.7, -3.1, -6.3, -11.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "fig = plot_all_data(data)\n",
    "plt.savefig('figures/all_perf_comparison.png', bbox_inches='tight', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
